





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import ConfusionMatrixDisplay





iris = pd.read_csv('data/iris.csv')


iris.head()


iris['species'].value_counts()





iris.dtypes


iris.isna().sum()





sns.scatterplot(iris, x= 'petal_length',y= 'petal_width', hue = 'species')





sns.pairplot(iris, corner = True, hue = 'species', diag_kind='hist');








! pip install plotly.express


import plotly.express as px


px.scatter_3d(iris, x = 'sepal_length', y = 'sepal_width', z = 'petal_length', color = 'species')





iris['species'].value_counts(normalize = True)








X = iris.drop(columns = 'species')
y = iris['species']


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state= 42, stratify= y)
# Stratify will make sure that the way the classes are seperated close to equally 





sc = StandardScaler()
# you need to instantiate it like a model


# fit + transform
X_train_sc = sc.fit_transform(X_train)


X_test_sc = sc.fit_transform(X_test)





knn = KNeighborsClassifier()





knn.fit(X_train_sc, y_train)


knn.score(X_train_sc, y_train)
# it is the accuracy for a classification model


knn.score(X_test_sc, y_test)


ConfusionMatrixDisplay.from_estimator(knn, X_test_sc, y_test, cmap = 'Blues')





bc = pd.read_csv('data/cancer.csv')


bc.head()


# Drop Unnamed: 32
bc.drop(columns = 'Unnamed: 32', inplace = True)


bc.info()








sns.pairplot(bc, 
            x_vars = ['texture_mean','radius_mean','concavity_mean','perimeter_worst','area_worst','symmetry_worst','concave points_mean'],
            y_vars = ['texture_mean','radius_mean','concavity_mean','perimeter_worst','area_worst','symmetry_worst','concave points_mean'],
            hue = 'diagnosis', corner = True)


bc['diagnosis'].value_counts()


bc['diagnosis'] = bc['diagnosis'].map({'M':1, 'B': 0})


plt.figure(figsize=(8,10))
sns.heatmap(bc.corr(numeric_only = True)[['diagnosis']].sort_values(by = 'diagnosis', ascending= False), 
           vmin= -1,
           vmax= 1, 
           annot = True, 
           cmap = 'coolwarm')





bc['diagnosis'].value_counts(normalize= True)








X = bc.drop(columns = ['id','diagnosis'])
y = bc['diagnosis']


X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(X, y, random_state= 42, stratify= y)





sc = StandardScaler()
X_train_bc_sc = sc.fit_transform(X_train_bc)
X_test_bc_sc = sc.transform(X_test_bc)





knn = KNeighborsClassifier()





# Fit
knn.fit(X_train_bc_sc, y_train_bc)


# Training accuracy score
knn.score(X_train_bc_sc, y_train_bc)


# Testing accuracy score
knn.score(X_test_bc_sc, y_test_bc)


# Confusion Matrix
ConfusionMatrixDisplay.from_estimator(knn, X_test_bc_sc, y_test_bc, cmap = 'Blues')





knn = KNeighborsClassifier(n_neighbors)

knn.fit(X_train_bc_sc, y_train_bc)

knn.score(X_test_bc_sc, y_test_bc)


scores = []

for k in range(3, 32, 2):
    knn = KNeighborsClassifier(n_neighbors = k)
    knn.fit(X_train_bc_sc, y_train_bc)
    train_score = knn.score(X_train_bc_sc, y_train_bc)
    test_score = knn.score(X_test_bc_sc, y_test_bc)

    scores.append({'k': k, 'train_score': train_score, 'test_score': test_score})

df_scores = pd.DataFrame(scores)


df_scores


# Visualize this:
sns.lineplot(df_scores, x = 'k', y = 'train_score', c = 'green', label = 'Train')
sns.lineplot(df_scores, x = 'k', y = 'test_score', c = 'red', label = 'test')
plt.xticks(range(3, 32, 2))


# Instantiate (again) & Refit (again)
knn = KNeighborsClassifier(n_neighbors= 7)
knn.fit(X_train_bc_sc, y_train_bc)


# New training score
knn.score(X_train_bc_sc, y_train_bc)


# New testing score
knn.score(X_test_bc_sc, y_test_bc)


# New Confusion Matrix
ConfusionMatrixDisplay.from_estimator(knn, X_test_bc_sc, y_test_bc, cmap ='Blues')


# A bit better!











from sklearn.linear_model import LogisticRegression


logr = LogisticRegression()
logr.fit(X_train_bc_sc, y_train_bc)


logr.score(X_train_bc_sc, y_train_bc)


logr.score(X_test_bc_sc, y_test_bc)


ConfusionMatrixDisplay.from_estimator(logr, X_test_bc_sc, y_test_bc, cmap = 'Reds');





from sklearn.ensemble import RandomForestClassifier


rf = RandomForestClassifier()
rf.fit(X_train_bc_sc, y_train_bc)


rf.score(X_train_bc_sc, y_train_bc)


rf.score(X_test_bc_sc, y_test_bc)


ConfusionMatrixDisplay.from_estimator(rf, X_test_bc_sc, y_test_bc, cmap = 'Purples');
